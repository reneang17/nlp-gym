{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using python to access web data (Excersices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.- Retrieve data using requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445833\n",
      "414217\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def count_numbers(text):\n",
    "    return sum([int(i) for i in re.findall('[0-9]+',text)])\n",
    "\n",
    "text = requests.get(\"https://py4e-data.dr-chuck.net/regex_sum_42.txt\")\n",
    "print(count_numbers(text.text))\n",
    "\n",
    "text = requests.get(\"https://py4e-data.dr-chuck.net/regex_sum_383060.txt\")\n",
    "print(count_numbers(text.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.- Using a socket to request data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r\n",
      "Date: Mon, 20 Apr 2020 18:40:44 GMT\r\n",
      "Server: Apache/2.4.18 (Ubuntu)\r\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\r\n",
      "ETag: \"1d3-54f6609240717\"\r\n",
      "Accept-Ranges: bytes\r\n",
      "Content-Length: 467\r\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\r\n",
      "Pragma: no-cache\r\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\r\n",
      "Connection: close\r\n",
      "Content-Type: text/plain\r\n",
      "\r\n",
      "Why should you learn to write programs?\n",
      "\n",
      "Writing programs (or programming) is a very creative \n",
      "and rewarding activity.  You can write programs for \n",
      "many reasons, ranging from making your living to solving\n",
      "a difficult data analysis problem to having fun to helping\n",
      "someone else solve a problem.  This book assumes that \n",
      "everyone needs to know how to program, and that once \n",
      "you know how to program you will figure out what you want \n",
      "to do with your newfound skills.  \n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "\n",
    "mysock.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2553\n"
     ]
    }
   ],
   "source": [
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = 'http://py4e-data.dr-chuck.net/comments_42.html'\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the span tags,searcd and print numbers\n",
    "def count_numbers(text):\n",
    "    return sum([int(i) for i in re.findall('[0-9]+',text)])\n",
    "\n",
    "tags = soup('span')\n",
    "count = 0\n",
    "for tag in tags:\n",
    "    count+= count_numbers(tag.decode())\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.- Using urllib to retrieve data and beautiful sour to parse html data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2441\n"
     ]
    }
   ],
   "source": [
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = 'http://py4e-data.dr-chuck.net/comments_383062.html'\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the span tags,searcd and print numbers\n",
    "def count_numbers(text):\n",
    "    return sum([int(i) for i in re.findall('[0-9]+',text)])\n",
    "\n",
    "tags = soup('span')\n",
    "count = 0\n",
    "for tag in tags:\n",
    "    count+= count_numbers(tag.decode())\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anayah\n"
     ]
    }
   ],
   "source": [
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import re\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "def follow_url(url):\n",
    "\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    return tags[2].get('href', None)\n",
    "\n",
    "url = 'http://py4e-data.dr-chuck.net/known_by_Fikret.html'\n",
    "for i in range(4):\n",
    "    url = follow_url(url)\n",
    "\n",
    "print(re.findall('by_(.+).html',url)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maaz\n"
     ]
    }
   ],
   "source": [
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import re\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "def follow_url(url):\n",
    "\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    return tags[17].get('href', None)\n",
    "\n",
    "url = 'http://py4e-data.dr-chuck.net/known_by_Charleigh.html'\n",
    "for i in range(7):\n",
    "    url = follow_url(url)\n",
    "\n",
    "print(re.findall('by_(.+).html',url)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.- User urllib to request xml and parse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/comments_42.xml\n",
      "Retrieved 4189 characters\n",
      "Count:  50\n",
      "2553\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "import ssl\n",
    "\n",
    "\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "\n",
    "    url =  'http://py4e-data.dr-chuck.net/comments_42.xml'\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "\n",
    "    data = uh.read()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "    #print(data.decode())\n",
    "    tree = ET.fromstring(data)\n",
    "\n",
    "    break\n",
    "\n",
    "lst = tree.findall('comments/comment')\n",
    "print(\"Count: \",len(lst))\n",
    "\n",
    "print(sum(int(item.find('count').text) for item in lst))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/comments_383064.xml\n",
      "Retrieved 4242 characters\n",
      "Count:  50\n",
      "Sum: 2392\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "import ssl\n",
    "\n",
    "\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "\n",
    "    url =  'http://py4e-data.dr-chuck.net/comments_383064.xml'\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "\n",
    "    data = uh.read()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "    #print(data.decode())\n",
    "    tree = ET.fromstring(data)\n",
    "\n",
    "    break\n",
    "\n",
    "lst = tree.findall('comments/comment')\n",
    "print(\"Count: \",len(lst))\n",
    "\n",
    "print(\"Sum: {}\".format( sum(int(item.find('count').text) for item in lst)) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.- Retriving a json with urllib and extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/comments_42.json\n",
      "2553\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "\n",
    "    url =  'http://py4e-data.dr-chuck.net/comments_42.json'\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "\n",
    "    data = uh.read().decode()\n",
    "\n",
    "    info = json.loads(data)\n",
    "\n",
    "    break\n",
    "\n",
    "count = 0\n",
    "for comment in info['comments']:\n",
    "    count+=comment['count']\n",
    "print(count)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/comments_383065.json\n",
      "2369\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "\n",
    "    url =  'http://py4e-data.dr-chuck.net/comments_383065.json'\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "\n",
    "    data = uh.read().decode()\n",
    "\n",
    "    info = json.loads(data)\n",
    "\n",
    "    break\n",
    "\n",
    "count = 0\n",
    "for comment in info['comments']:\n",
    "    count+=comment['count']\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.- Retriving data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/json?address=University+of+Florida&key=42\n",
      "Retrieved 1820 characters\n",
      "lat 29.6436325 lng -82.3549302\n",
      "->>> ChIJdZLk-wyj6IgRhpg9FN7gbyA\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = 'University of Florida'#input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "\n",
    "    #print(json.dumps(js, indent=4))\n",
    "\n",
    "    lat = js['results'][0]['geometry']['location']['lat']\n",
    "    lng = js['results'][0]['geometry']['location']['lng']\n",
    "    print('lat', lat, 'lng', lng)\n",
    "    location = js['results'][0]['formatted_address']\n",
    "    for result in js['results']:\n",
    "        print('->>>', result['place_id'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
